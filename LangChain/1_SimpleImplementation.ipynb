{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/GenAI/blob/main/1_SimpleImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTYh_jRouNaK",
        "outputId": "efb1831e-3a85-4c7d-d636-e90053f29d40"
      },
      "id": "UTYh_jRouNaK",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.47.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.47.1-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.47.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8973557-8145-498d-a3de-d5b90e9bf86e",
      "metadata": {
        "id": "a8973557-8145-498d-a3de-d5b90e9bf86e"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "#openai = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "jBxpiFYMuXFY"
      },
      "id": "jBxpiFYMuXFY",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5e411bb-50f1-4163-b454-c3b4a9bffef2",
      "metadata": {
        "id": "b5e411bb-50f1-4163-b454-c3b4a9bffef2"
      },
      "source": [
        "# Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Creating OpenAI Instance\n",
        "client = OpenAI(api_key = userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  # Here I am using Gpt-40-mini model. You can use anyother text model\n",
        "  model=\"gpt-4o-mini\",\n",
        "  # Here we can also specify assistant role in messages or we can provide few shot learning example\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is a LLM?\"}\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "8AaharSn5xqB"
      },
      "id": "8AaharSn5xqB",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6e517be5-8d3f-48b2-8c72-414b4e48f302",
      "metadata": {
        "id": "6e517be5-8d3f-48b2-8c72-414b4e48f302",
        "outputId": "ef8db6bd-323d-4c78-a415-986c183ecb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-AB1hd3RztS32L5QOZGc2htfZ6GtjP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"A LLM, or Large Language Model, is a type of artificial intelligence model that is designed to understand and generate human language. These models are trained on vast amounts of text data and utilize advanced machine learning techniques, particularly deep learning, to learn the patterns and structures of language.\\n\\nKey characteristics of LLMs include:\\n\\n1. **Scale**: LLMs are typically very large, often consisting of billions of parameters, which allows them to capture complex language features and relationships.\\n\\n2. **Training**: They are trained on diverse datasets, which can include books, articles, websites, and other textual data, allowing them to develop a broad understanding of various topics and writing styles.\\n\\n3. **Applications**: LLMs are used in a wide range of applications, including text generation, translation, summarization, conversation agents (chatbots), sentiment analysis, and more.\\n\\n4. **Examples**: Some well-known examples of LLMs include OpenAI's GPT series (like GPT-3 and GPT-4), Google's BERT and T5, and Facebook's BART.\\n\\n5. **Contextual Understanding**: LLMs use techniques like attention mechanisms to understand context and maintain coherence in the generated text, making them capable of generating responses that are relevant to input prompts or queries.\\n\\nOverall, LLMs represent a significant advancement in natural language processing (NLP) and have made it possible to automate and improve many language-based tasks.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727191517, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=292, prompt_tokens=23, total_tokens=315, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we only wanna see the output\n",
        "message = response.choices[0].message.content"
      ],
      "metadata": {
        "id": "z-m0oJ4uo2Ds"
      },
      "id": "z-m0oJ4uo2Ds",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "GGi-ZR04pDQP",
        "outputId": "354f5538-f5a9-481e-87a9-df2bda8c9a6f"
      },
      "id": "GGi-ZR04pDQP",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A LLM, or Large Language Model, is a type of artificial intelligence model that is designed to understand and generate human language. These models are trained on vast amounts of text data and utilize advanced machine learning techniques, particularly deep learning, to learn the patterns and structures of language.\\n\\nKey characteristics of LLMs include:\\n\\n1. **Scale**: LLMs are typically very large, often consisting of billions of parameters, which allows them to capture complex language features and relationships.\\n\\n2. **Training**: They are trained on diverse datasets, which can include books, articles, websites, and other textual data, allowing them to develop a broad understanding of various topics and writing styles.\\n\\n3. **Applications**: LLMs are used in a wide range of applications, including text generation, translation, summarization, conversation agents (chatbots), sentiment analysis, and more.\\n\\n4. **Examples**: Some well-known examples of LLMs include OpenAI's GPT series (like GPT-3 and GPT-4), Google's BERT and T5, and Facebook's BART.\\n\\n5. **Contextual Understanding**: LLMs use techniques like attention mechanisms to understand context and maintain coherence in the generated text, making them capable of generating responses that are relevant to input prompts or queries.\\n\\nOverall, LLMs represent a significant advancement in natural language processing (NLP) and have made it possible to automate and improve many language-based tasks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here we can use text models for code generation, grammar correction, document drafts, translation tasks etc. I am not doing all of them because these all are pretty straight forward."
      ],
      "metadata": {
        "id": "G1M3TQG5p_kT"
      },
      "id": "G1M3TQG5p_kT"
    },
    {
      "cell_type": "markdown",
      "id": "b7672584-3a25-4140-920c-3c5f72ce6d48",
      "metadata": {
        "id": "b7672584-3a25-4140-920c-3c5f72ce6d48"
      },
      "source": [
        "# Open AI Chat Completion API Parameters:\n",
        "\n",
        "## Required parameters:\n",
        "1. **messages**\n",
        "2. **model**\n",
        "\n",
        "## Optional Parameters:\n",
        "1. **max_tokens**:  It specifies the maximum number of tokens allowed to GPT for generating the response. It will stop generating responses after a certain number of tokens. The default value is 4096 tokens. The total length of input tokens and generated tokens is limited by the model's context length.\n",
        "2. **temperature**: It adjusts the model's level of creativity, from straightforward to imaginative. The default value is 1, it can be set to 0 or 2. Temperature affects the sampling choice of the next predicted tokens. In short, the lower the temperature, the more deterministic the results, meaning the highest probable next token is always chosen. Increasing the temperature could lead to more randomness, encouraging more diverse or creative outputs.\n",
        "3. **top_p**:  top-p or nucleus sampling and temperature serve the same purpose in language models. Top-p sampling selected tokens from the smallest set whose cumulative probability surpasses a given threshold, denoted as ‘p’. Rather than sampling from the entire distribution of possible tokens, top-p sampling focuses on a subset with the highest probabilities. The default value is 1 and setting top-p to 0.1 means only tokens in the top 10% probability range are considered. It is not recommended to use temperature and top-p together.\n",
        "4. **frequency_penalty**: This parameter helps to reduce the repeated sampling of the same set of tokens. It instructs the language model to not use the same words too frequently. The mechanism works by directly adjusting the logits (un-normalized log probabilities) with an additive contribution. The default value is 0, and suitable penalty coefficients typically range from 0.1 to 1. A higher value means the model will be less inclined to reuse the same words, while a lower value implies a greater likelihood of repetitive word usage.\n",
        "5. **presence_penalty**: It can be used to encourage the model to use a diverse range of tokens in the generated text. It instructs the language model to utilize different words, promoting variety in the outputs. A higher presence_penalty value will result in the model being more likely to generate tokens that have not yet been included in the generated text. A higher presence_penalty value will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
        "6. **logit_bias**: It acts as a tool to stop the model from generating specific, unwanted tokens or words. By using this parameter, you can control or exclude certain words from GPT’s output. It takes a JSON object that links token IDs to associated bias values ranging from -100 to 100. Essentially, this bias is mathematically added to the logits produced by the model before the sampling process.\n",
        "In short, assigning -100 to a selected token will effectively ban that token from being generated, while assigning 100 will ensure an exclusive selection of that token in the generated output.\n",
        "7. **seed**: seed is still in a beta feature, but it allows you to obtain consistent results for every input submitted to GPT. For instance, if you ask GPT to narrate a story about Pakist'san Political Situation, the generated story may vary each time you ask. However, using the seed will prompt the system to make its best attempt at deterministic sampling, ensuring you get the same result for a given input.\n",
        "8. **function_call(Deprecated)**: This parameter allows you to instruct the language model to perform a specific task before generating an answer to the user’s input. A common example is a weather application where the model lacks current weather data. In such cases, you can gather information from a weather API, incorporate that data into the prompt context, and then instruct the model to generate an answer using that context.\n",
        "9**logprobs**: Its value can be boolean or null. It specifies whether to return the log probabilities of the output token or not. If it is set to True, it will return log probabilities of each output token returned in the content of messages. Its default value is False.\n",
        "10. **top_logprobs**: Its value can be integer or null. An integer between 0 and 20 specifies the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n",
        "11. **n**: It can be integer or null. How many chat completion choices to generate for each input message?\n",
        "12. **response_format**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "41895279-7214-4e42-9aac-1088179a2f2e",
      "metadata": {
        "id": "41895279-7214-4e42-9aac-1088179a2f2e",
        "outputId": "0e44d2d8-c752-4020-825d-025112abc247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I can't provide real-time weather updates or current conditions. You can check a weather website or app for the most accurate and current weather information. Let me know if you need help with anything else!\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dcf7425c-5a81-4b20-84fe-9799a780b2a9",
      "metadata": {
        "id": "dcf7425c-5a81-4b20-84fe-9799a780b2a9",
        "outputId": "8a5a050a-85f6-4e5f-a3ed-1f4e4917c1e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imran Khan is a prominent Pakistani politician, former cricketer, and philanthropist who currently serves as the Prime Minister of Pakistan. He founded\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What do you think about imran khan? Tell me in detail\"}\n",
        "    ],\n",
        "    max_tokens=30\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0b88f5e6-670d-48a5-89f2-970297829f8a",
      "metadata": {
        "id": "0b88f5e6-670d-48a5-89f2-970297829f8a",
        "outputId": "7b161a4f-ffe7-496a-e59c-0945857bebc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imran Khan is a prominent figure in Pakistan, known for his multifaceted career as a cricketer, philanthropist, and politician. He gained international fame as a cricketer, leading the Pakistan national team to its first and only Cricket World Cup victory in 1992. After retiring from cricket, he founded the Shaukat Khanum Memorial Cancer Hospital & Research Centre, which reflects his commitment to philanthropy and healthcare.\n",
            "\n",
            "In politics, Imran Khan founded the Pakistan Tehreek-e-Insaf (PTI) party and served as the Prime Minister of Pakistan from August 2018 until April 2022. His tenure was marked by efforts to combat corruption, improve the economy, and address social issues, but it also faced challenges, including economic difficulties and political opposition.\n",
            "\n",
            "Public opinion about him is polarized; some view him as a reformist leader striving for change, while others criticize his governance and economic policies. His political journey has been dynamic, and he remains a significant figure in Pakistan's political landscape.\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What do you think about imran khan?\"}\n",
        "    ],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "06fa3aed-6fcf-4f9d-8b0b-f14b449b1b33",
      "metadata": {
        "id": "06fa3aed-6fcf-4f9d-8b0b-f14b449b1b33",
        "outputId": "675c44cb-feb2-4b1d-bfc6-c0f4da8476c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imran Khan is a prominent figure in Pakistan, known for his multifaceted career as a cricketer, philanthropist, and politician. He gained international fame as a cricketer, leading the Pakistan national team to its first and only Cricket World Cup victory in 1992. After retiring from cricket, he founded the Shaukat Khanum Memorial Cancer Hospital & Research Centre, which reflects his commitment to philanthropy and healthcare in Pakistan.\n",
            "\n",
            "In politics, Imran Khan founded the Pakistan Tehreek-e-Insaf (PTI) party and served as the Prime Minister of Pakistan from August 2018 until April 2022. His tenure was marked by various challenges, including economic issues, foreign relations, and political opposition. Supporters praise him for his anti-corruption stance and efforts to improve social welfare, while critics point to economic difficulties and governance challenges during his time in office.\n",
            "\n",
            "Overall, Imran Khan is a polarizing figure, admired by many for his achievements in sports and philanthropy, while also facing significant criticism in the political arena. His legacy continues to evolve as he remains an influential figure in Pakistan's political landscape.\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What do you think about imran khan?\"}\n",
        "    ],\n",
        "    top_p=0.1\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69639c83-3c0d-4c37-90b9-f921ff0b31dc",
      "metadata": {
        "id": "69639c83-3c0d-4c37-90b9-f921ff0b31dc"
      },
      "source": [
        "# Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all we can generate a new image giving relevant prompt.\n",
        "# Here you are charged on basis of model you are using and on size of image.\n",
        "\n",
        "response = client.images.generate(\n",
        "  # Specifying model I wanna use\n",
        "  model=\"dall-e-3\",\n",
        "  # Giving prompt\n",
        "  prompt=\"a pakistani girl in a park\",\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  # n specifies number of images we wanna generate for above mentioned prompt\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "# This will give url for image\n",
        "image_url = response.data[0].url"
      ],
      "metadata": {
        "id": "fevJ1wzgvWZB"
      },
      "id": "fevJ1wzgvWZB",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ab1d06a3-930c-429b-9c46-c1f8da8b8a30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1d06a3-930c-429b-9c46-c1f8da8b8a30",
        "outputId": "60eaf5c7-2910-4c74-a945-e7683fb61768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://oaidalleapiprodscus.blob.core.windows.net/private/org-pm94GjiHwe3RW0IEvPv6reti/user-qbvOFUj5H9MOgljxjMbKTHh8/img-p8VrJ4EgNnhR5KHfRPsWwWeZ.png?st=2024-09-24T14%3A44%3A20Z&se=2024-09-24T16%3A44%3A20Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-09-23T23%3A21%3A51Z&ske=2024-09-24T23%3A21%3A51Z&sks=b&skv=2024-08-04&sig=egaI1Sf/R/fZsGSdhA8243C/8ZcsXMN%2BHSzrm59L1zs%3D\n"
          ]
        }
      ],
      "source": [
        "print(image_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can give images as input and ask model anything about that image/images, it can also tell difference in both images"
      ],
      "metadata": {
        "id": "ZsOwVGKPuL_C"
      },
      "id": "ZsOwVGKPuL_C"
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  # Specify model, here using gpt-4o-mini, it can take images, text, audio as input and give text as output\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "          # In text we will specify what we wanna know about that image\n",
        "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
        "            \"detail\": \"high\"\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  max_tokens=300,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEVO4dfGujU0",
        "outputId": "a3d0aeee-79a4-411f-8641-0647f2ff821f"
      },
      "id": "pEVO4dfGujU0",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image shows a serene landscape featuring a wooden pathway that runs through a lush, grassy area. There are vibrant green grasses on either side of the path, and the sky above is blue with soft, wispy clouds. The setting appears to be natural and peaceful, possibly indicative of a wetland or nature reserve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d3b8ce-146e-4400-a986-ff04e5145dfe",
      "metadata": {
        "id": "86d3b8ce-146e-4400-a986-ff04e5145dfe"
      },
      "source": [
        "# Voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "972b65a4-e8e5-41b1-b999-11e381b272f0",
      "metadata": {
        "id": "972b65a4-e8e5-41b1-b999-11e381b272f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f48d24-fa86-43b7-a2dd-87dc9df2e69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max's ordinary day took an unexpected turn when he stumbled upon a glowing map hidden under his bed. It's mysterious...\n"
          ]
        }
      ],
      "source": [
        "audio_file= open(\"/content/audio.mp3\", \"rb\")\n",
        "transcription = client.audio.transcriptions.create(\n",
        "  model=\"whisper-1\",\n",
        "  file=audio_file\n",
        ")\n",
        "print(transcription.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here in translation we can use audio in different language to be translated in English\n",
        "# I don't have any audio so I used the english one only.\n",
        "\n",
        "audio_file= open(\"/content/audio.mp3\", \"rb\")\n",
        "translation = client.audio.translations.create(\n",
        "  model=\"whisper-1\",\n",
        "  file=audio_file\n",
        ")\n",
        "print(translation.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFiVTqLmSj4n",
        "outputId": "6ba72578-490a-45b4-d557-2b49440bb064"
      },
      "id": "AFiVTqLmSj4n",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max's ordinary day took an unexpected turn when he stumbled upon a glowing map hidden under his bed. Its mysterious...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zw25lTrdSxwm"
      },
      "id": "zw25lTrdSxwm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}