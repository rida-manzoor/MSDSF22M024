{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTdKnxAFcnXqSIl8xmedB0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/17_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agenda of this Notebook\n",
        "1. Why we can't train our own model?\n",
        "2. What is transfer Learning?\n",
        "3. Problem with pre-trained models.\n",
        "4. Why transfer learning works?"
      ],
      "metadata": {
        "id": "oTZp_tA6eoe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why we can't train our own model?\n",
        "1. Neural Network are data hungary\n",
        "2. Computation costs\n",
        "3. Data labelling is an issue.\n",
        "\n",
        "\n",
        "## Problem with pre-trained models\n",
        "- **Generalization Limitations:** Pre-trained models may not generalize well to specific or niche domains.\n",
        "\n",
        "- **Biases:** Models can inherit biases from the data they were trained on, leading to biased outputs.\n",
        "\n",
        "- **Lack of Domain Knowledge:** Models lack domain-specific knowledge and may not perform optimally in specialized fields.\n",
        "\n",
        "- **Overfitting to Training Data:** Models might overfit to biases present in the training data, limiting adaptability to new contexts.\n",
        "\n",
        "- **Computational Resources:** Fine-tuning or adapting large pre-trained models requires substantial computational resources.\n",
        "\n",
        "# Transfer Learning\n",
        "Transfer learning is a machine learning technique where a model trained on one task is adapted for a related but different task. Instead of training a model from scratch for a new task, transfer learning leverages knowledge gained from solving one problem to improve performance on another. There are two main types of transfer learning:\n",
        "\n",
        "1. **Feature-based Transfer Learning:** The pre-trained model's lower layers (features) are used as a generic feature extractor for the new task. The higher layers are then retrained for the specific task. Used for task in which labels are kind of similar to data on which model is already trained.\n",
        "\n",
        "2. **Fine-tuning:** The entire pre-trained model is further trained on the new task, adjusting weights across all layers to adapt to the specific domain. Used for tasks whose labels are different from data labels on which model is trained.\n",
        "\n",
        "Transfer learning is particularly useful when the amount of labeled data for the target task is limited, as it enables the model to leverage knowledge gained from larger datasets in related domains."
      ],
      "metadata": {
        "id": "V0BDbgSne6fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why transfer learning work?\n",
        "- Early layers only extract primitive features(Edges). And last layer classify image. So Early layer task are generic and will work same for every image. That's why we don't replace them."
      ],
      "metadata": {
        "id": "uT2o-Id2XC0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Benefits of Transfer Learning:\n",
        "\n",
        "**Improved Performance:** Transfer learning can lead to improved performance on the target task, especially when the source and target tasks are related or have some common features.\n",
        "\n",
        "**Faster Convergence:** By starting with pre-trained weights and learned features, transfer learning can accelerate the training process and reduce the amount of data required for training.\n",
        "\n",
        "**Effective Use of Limited Data:** Transfer learning is particularly useful when the target dataset is small or limited, as it can leverage knowledge from larger or more diverse datasets."
      ],
      "metadata": {
        "id": "kA-m3jfqgi4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the pre-trained VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add new classification layers on top of the base model\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create the transfer learning model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define data generators for training and validation data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train_data_directory',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    'train_data_directory',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples,\n",
        "    epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "id": "s-IU2GuvYz4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}