{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORY1FRpYiCUUduDJPF5YYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/26_Attention_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanism\n",
        "\n",
        "Why we need attention mechanism?\n",
        "\n",
        "1. Encoder first read whole sentence and then generate a summary. If the sentence is long,like paragraph, remembering whole sentence is kinda difficult. Thats mean we are creating unneccessary pressure on encode to do so. This is the first problem with encoder decoder.\n",
        "2. In decoder part, at given timestep we don't need whole sentence to translate particular word/token. Because of this statis representation decoder face problem to decode.\n",
        "3. BLEU Score start decreasing when we increase length of string.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "We have to introduce attention space in decoder. At a particular time step we need to tell decoder that this timestep from encoder is most important to translate given timestep.\n",
        "\n",
        "![ds](https://miro.medium.com/v2/resize:fit:640/format:webp/1*We87td0yKdnI_pDGkyvS1g.png)\n",
        "\n",
        "c_i is vector and dimension is exactly same as h_i. But if we have to give more than one hidden state we will give weighted average.\n",
        "\n",
        "$$c_1 = α_1h_1+α_2h_2+α_ih_i  $$\n",
        "$$c_2 = α_1h_1+α_2h_2+α_ih_i  $$\n",
        "$$c_3 = α_1h_1+α_2h_2+α_ih_i  $$\n",
        "$$c_i=Σα_ijh_i $$\n",
        "\n",
        " alpha_21 is the similarity score that tells at decoder 2 timestep, what role encoder 1 timestep have. And it depends on h_1 and s_1(which is previous hidden state of decoder)\n",
        "\n",
        " $$α_ij=f(h_j,s_i-1)  $$\n",
        "\n",
        " Now what function is more suitable for approximation???\n",
        "\n",
        " Here researcher use ANN as function. As ANN is universal function approximator if provided enough data. So there is a dense neural network whose input is h_i and s_i & will give alpha as output. Sum of all apha will always be one.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Onz5TSMp3Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do we find out alignment score (Alpha)?\n",
        "\n",
        "1. Bahdanau Attention\n",
        "2. Luong Attention\n",
        "\n",
        "\n",
        "\n",
        "**What are we trying to do?**\n",
        "$$c_i=Σα_ijh_i $$\n",
        "\n",
        "We want to find out these alignment scores which actually tells us about the input (Contribution, weightage) of any hidden state of encoder for any timestep of decoder. It tells us that given at the output that is printed till now, current decoder timestep output, how much depend on all hidden states of encoder.\n",
        "\n",
        "**α depends on**\n",
        "* Decoder previous hidden state\n",
        "* Hidden state of encoder(of that timestep)\n",
        "\n",
        "So, Alignment score will be the mathematical function of two things, encoder hidden state and decoder previous hidden state."
      ],
      "metadata": {
        "id": "Q2625u4XG8Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bahdanau Attention\n",
        "In this attention, we give a vector to feed forward network which is combination of encoder hidden states and decoder previous hidden states. Here suppose feed forward network we gonna use has one layer of fully connected neurons and one output layer(which have one neuron).\n",
        "\n",
        "Let's Assume we wanna to find:\n",
        "$$c_i=Σα_ijh_i $$\n",
        "\n",
        "and we know α depends on $h_j$ and $s_(i-1)\n",
        "\n",
        "\n",
        "$$h_0 = [a,b,c,d]$$\n",
        "if we have four hidden states, we will get $ h_0, h_1, h_2, h_3 $\n",
        "\n",
        "Similarly, $$s_0 = [a,b,c,d]$$\n",
        "\n",
        "\n",
        "If we are on first time step, FFN input will be the matrix where we will concate $s_0$ with $h_0, h_1, h_2, h_3$. It will give a matrix having 4 rows and 8 columns.\n",
        "\n",
        "$$ [s_00, s_01, s_02, s_03, h_00, h_01, h_02, h_03]  $$\n",
        "\n",
        "\n",
        "We can give each row as a seperate input to FFN or do in batch operation.\n",
        "\n",
        "When we go on next timestep of decoder, we will use $S_1$ instead and repeat whole process.\n",
        "\n",
        "Output layer will give a vector with 4 values.\n",
        "$$[e_11, e_12, e_13, e_14]$$\n",
        "\n",
        "Lastly, softmax function will be used to normalize it. and these values are the values of alpa we gonna use for that timestep. This softmax will transform values so that sum of all alph values will be one. This FFN is called time distributed FFN.\n",
        "\n",
        "\n",
        "$$ e_ij = v tanh(w[s_i-1 : h_j]+b)$$"
      ],
      "metadata": {
        "id": "pH_51ZDHJDiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Luong Attention\n",
        "\n",
        "Purpose of Luong attention is same as Bahdanau attention. But it use different function to calculate alpha.\n",
        "\n",
        "1. It use current hidden state instead of previous hidden states of decoder\n",
        "2. Instead of creating FFN, it only do dot product of encoder hidden state and current decoder hidden state.\n",
        "\n",
        "\n",
        "It improves performance also it is faster than Bahdanau because we don't have so many parameters. It used current state because it is the most updated information."
      ],
      "metadata": {
        "id": "zrBQ_0LdanLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Yhsgizow-E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "* https://distill.pub/2016/augmented-rnns/\n",
        "* https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc"
      ],
      "metadata": {
        "id": "2t_CPfLdzZbs"
      }
    }
  ]
}