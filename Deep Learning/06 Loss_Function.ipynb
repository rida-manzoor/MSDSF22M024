{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/Loss_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOnFEyar-Pne"
   },
   "source": [
    "# **What is loss function**\n",
    "\n",
    "It is a method of evaluating how well your algorithm is modelling your dataset. If output of loss function is high, model is not behaving well. It should be low.\n",
    "A loss function, also known as a cost function or objective function, is a crucial component. Its primary role is to measure the difference between the predicted output of a model and the actual ground truth labels or targets. The goal of the loss function is to quantify how well or poorly the model is performing on a given dataset, providing feedback that guides the optimization process during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DISg21bcABWG"
   },
   "source": [
    "**Loss Function in DL**\n",
    "Some main types of loss functions are:\n",
    "\n",
    "1. **Regression**\n",
    "\n",
    "    1. MSE\n",
    "    2. MAE\n",
    "    3. B. Huber loss\n",
    "2. **Classification**\n",
    "\n",
    "    1. Binary Crossentropy\n",
    "    2. Categorical Crossentropy\n",
    "    3. Hinge Loss\n",
    "\n",
    "3. **AutoEncoder**\n",
    "    1. KL Diverfence\n",
    "\n",
    "4. **GAN**\n",
    "    1. Discriminator loss\n",
    "    2. MinMax Gan loss\n",
    "\n",
    "5. **Embeddings**\n",
    "    1. Tripled Loss\n",
    "\n",
    "6. **Object Detection**\n",
    "    1. Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAQBaqd2B-At"
   },
   "source": [
    "**Loss Function vs Cost Function**\n",
    "\n",
    ">  **Loss function:** Used when we refer to the error for a single training example. **Cost function:** Used to refer to an average of the loss functions over an entire training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaoglcYkCsAg"
   },
   "source": [
    "## **Mean Squared Error / L2 error**\n",
    "\r\n",
    "MSE stands for Mean Squared Error, which is a commonly used loss function in regression tasks. It measures the average squared difference between the predicted values and the actual target values in a dataset. The MSE is calculated by taking the average of the squared differences between each predicted value and its corresponding actual value.\n",
    "\n",
    "$$ (y_i - ̂y_i)^2  $$a*ble.\n",
    "\n",
    "\n",
    "*Adv*antage*\n",
    "1. Easy to interpret\n",
    "2. Always differentiable\n",
    "3. One local m*inima\n",
    "\n",
    "*Disadv*antage*\n",
    "1. Error unit is squared\n",
    "2. It is not robust to outliers\n",
    "\n",
    ">The aActivation function ofthe  last neuron should be linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points about MSE:\n",
    "\n",
    "- **Squared Differences**: MSE calculates the squared difference between each predicted value and its corresponding actual value. Squaring the differences penalizes larger errors more heavily than smaller errors.\n",
    "  \n",
    "- **Non-Negative Value**: MSE is always non-negative since it involves squaring the differences. A value of 0 indicates perfect predictions, where the predicted values exactly match the actual values.\n",
    "- **Loss Function**: In machine learning models, MSE is often used as a loss function during training. The goal is to minimize the MSE, which means reducing the average squared difference between predictions and targets.\n",
    "- **Impact of Outliers**: MSE can be sensitive to outliers in the data, as larger errors (due to outliers) contribute significantly to the overall loss.\n",
    "- **Interpretation**: The square root of MSE (RMSE, Root Mean Squared Error) is sometimes used for easier interpretation, as it is in the same scale as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YrWyAjD6uL"
   },
   "source": [
    "## **Mean Absolute error / L1 error**\n",
    "\n",
    "\r\n",
    "Mean Absolute Error (MAE) is another commonly used loss function in regression tasks, similar to Mean Squared Error (MSE). However, unlike MSE which calculates the average squared difference between predicted and actual values, MAE calculates the average absolute difference. This means that MAE measures the average magnitude of errors without considering their direction (positive or negative).\n",
    "\n",
    "$$ |y_i - ŷ_i| $$\n",
    "\n",
    "\n",
    "*Advantages*\n",
    "1. Easy to understand\n",
    "2. Unit is same as 'y'\n",
    "3. Robust to outliers\n",
    "\n",
    "*Disadvantages*\n",
    "1. Not differentiable (have to calculate sub-gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points about MAE:\n",
    "\n",
    "- **Absolute Differences**: MAE calculates the absolute difference between each predicted value and its corresponding actual value. This means that errors are not squared, and both positive and negative errors contribute equally to the overall loss.\n",
    "- **Robustness to Outliers**: MAE is more robust to outliers compared to MSE, as it does not heavily penalize large errors. Outliers have a linear impact on MAE, unlike MSE where their impact is quadratic.\n",
    "- **Interpretation**: The average absolute difference calculated by MAE is easier to interpret in the context of the problem, as it represents the average magnitude of errors in the predictions.\n",
    "- **Loss Function**: Like MSE, MAE can also be used as a loss function during training regression models. The goal is to minimize the MAE, indicating a reduction in the average absolute difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxepOl9CEnoM"
   },
   "source": [
    "## **Huber Loss**\n",
    "\n",
    "Huber Loss, also known as Smooth Mean Absolute Error, is a loss function used in regression tasks that combines the benefits of Mean Absolute Error (MAE) and Mean Squared Error (MSE). It is designed to be more robust to outliers compared to MSE while still maintaining the smoothness properties of MAE.\n",
    " \n",
    " $$ {\\displaystyle L_{\\delta }(a)={\\begin{cases}{\\frac {1}{2}}{y-ŷ}&{\\text{for }}|y-ŷ|\\leq \\delta ,\\\\\\delta \\cdot \\left(|y-ŷ |-{\\frac {1}{2}}\\delta \\right),&{\\text{otherwise.}}\\end{cases}}}  $$\n",
    "\n",
    "If the data point is an outlier, Huber will behave like MAE, if it is not an outlier, Huber will behave like MSE. **Sigma** is a hyperparameter which will determine whether the datapoint is an outlier or not.\n",
    "\n",
    "Key points about Huber loss:\r\n",
    "1. **Smooth Transition:** Huber loss smoothly transitions between the quadratic loss (MSE) for small errors and the linear loss (MAE) for large errors. This makes it less sensitive to outliers while still penalizing large errors effectively.\r\n",
    "2. **Threshold Paramete\\( \\del )):** The choice of \\( \\delta \\) affects the behavior of the loss function. A smaller \\( \\delta \\) makes Huber loss more robust to outliers but may result in slower convergence, while a larger \\( \\delta \\) can lead to faster convergence but may be less robust to outliers.\r\n",
    "3. **Differentiable:** Huber loss is differentiable everywhere, including at the point where it switches between the quadratic and linear components. This property is beneficial for gradient-based optimization algorithms used in training neural networks.\r\n",
    "4. **Usage:** Huber loss is commonly used in machine learning models, especially in situations where the dataset contains noisy or outlier-prone data. It strikes a balance between robustness to outliers and convergence speed during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG3EqHGbGO6H"
   },
   "source": [
    "## **Binary Crossentropy / log Loss**\n",
    "\n",
    "Binary Cross-Entropy Loss (also known as Log Loss or Binary Log Loss) is a loss function commonly used in binary classification tasks. It measures the difference between the predicted probabilities and the actual binary labels (0 or 1) for each sample in the dataset. The goal is to minimize this difference, which is crucial for training accurate binary classification models.\n",
    "\n",
    "$$-ylog(ŷ) -(1-y)log(1-ŷ)  $$\n",
    "\n",
    "> Last activation function should be Sigmoid.\n",
    "\n",
    "*Advantage*\n",
    "1. Differentiable\n",
    "\n",
    "*Disadvantage*\n",
    "1. Multiple Local minima\n",
    "2. Not Intuitive\n",
    "ities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points about Binary Cross-Entropy Loss:\n",
    "\n",
    "- **Probability-Based Loss**: Binary Cross-Entropy Loss is based on the predicted probabilities of the positive class (class 1) for each sample. It penalizes the model more heavily for incorrect predictions that are confidently wrong (high predicted probability for the wrong class).\n",
    "- **Logarithmic Function**: The use of logarithmic functions in the loss formula ensures that the loss increases exponentially as the predicted probability diverges from the actual binary label. This amplifies the impact of confident incorrect predictions.\n",
    "- **Differentiability**: The Binary Cross-Entropy Loss function is differentiable with respect to the model's parameters, making it suitable for gradient-based optimization algorithms like stochastic gradient descent (SGD) during model training.\n",
    "- **Suitability**: Binary Cross-Entropy Loss is well-suited for binary classification tasks where the goal is to classify inputs into two mutually exclusive classes (e.g., positive/negative, yes/no, spam/not spam).\n",
    "- **Output Activation**: Typically, the output layer of the neural network in binary classification tasks uses a sigmoid activation function, which ensures that the predicted values are in the range [0, 1] representing probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Categorical cross entropy**\n",
    "\n",
    "Categorical Cross-Entropy Loss (also known as Multiclass Cross-Entropy Loss) is a loss function commonly used in multiclass classification tasks. It measures the difference between the predicted class probabilities and the actual one-hot encoded class labels for each sample in the dataset. The goal is to minimize this difference, which is essential for training accurate multiclass classification models.\n",
    "\n",
    "$$ - ∑ y_j log ŷ_j $$\n",
    "\n",
    "> Activation fun should be softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOLhp3s0-IQw"
   },
   "source": [
    "Key points about Categorical Cross-Entropy Loss:\n",
    "\n",
    "- **Probability-Based Loss**: Categorical Cross-Entropy Loss is based on the predicted probabilities of each class for each sample. It penalizes the model more heavily for incorrect predictions that are confidently wrong (high predicted probability for the wrong class).\n",
    "- **Logarithmic Function**: The use of logarithmic functions in the loss formula ensures that the loss increases exponentially as the predicted probability diverges from the actual class label. This amplifies the impact of confident incorrect predictions.\n",
    "- **Differentiability**: The Categorical Cross-Entropy Loss function is differentiable with respect to the model's parameters, making it suitable for gradient-based optimization algorithms like stochastic gradient descent (SGD) during model training.\n",
    "- **Suitability**: Categorical Cross-Entropy Loss is well-suited for multiclass classification tasks where the goal is to classify inputs into multiple mutually exclusive classes (e.g., different types of objects, sentiment categories).\n",
    "- **Output Activation**: Typically, the output layer of the neural network in multiclass classification tasks uses a softmax activation function, which ensures that the predicted values are probabilities that sum up to 1 across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hinge Loss**\n",
    "\n",
    "Hinge Loss is a loss function commonly used in binary classification tasks, especially in the context of support vector machines (SVMs) and margin-based classifiers. It is designed to penalize the model based on the margin between the predicted decision boundary and the true class labels. Hinge Loss encourages the model to correctly classify samples with a margin greater than a specified threshold, promoting better separation between classes.\n",
    "\n",
    "\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i) $$\n",
    "\n",
    "In this formula:\n",
    "- \\( L(y, \\hat{y}) \\) represents the Hinge Loss between the true labels \\( y \\) and the predicted scores \\( \\hat{y} \\).\n",
    "- \\( N \\) is the number of samples in the dataset.\n",
    "- \\( y_i \\) is the true class label for the \\( i \\)-th sample, typically -1 or 1 in binary classification tasks.\n",
    "- \\( \\hat{y}_i \\) is the predicted score (output of the decision function) for the \\( i \\)-th sample.\n",
    "- \\( \\max(0, 1 - y_i \\cdot \\hat{y}_i) \\) calculates the hinge loss for each sample, penalizing the model based on the margin between the predicted score and the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points about Hinge Loss:\n",
    "\n",
    "- **Margin-Based Loss**: Hinge Loss is a margin-based loss function that penalizes the model when the predicted score (decision function output) for a sample is within a certain margin of the true class label. The margin is typically set to 1 in binary classification tasks.\n",
    "- **Sparse Margin**: Hinge Loss encourages a sparse margin between classes, meaning that the model aims to correctly classify samples with a margin greater than 1 while ignoring correctly classified samples that are already sufficiently far from the decision boundary.\n",
    "- **Support Vector Machines (SVMs)**: Hinge Loss is commonly used in SVMs as the optimization objective for maximizing the margin between classes while minimizing classification errors.\n",
    "- **Robust to Outliers**: Hinge Loss is less sensitive to outliers compared to other loss functions like Cross-Entropy Loss. Outliers that are correctly classified with a large margin do not contribute significantly to the loss.\n",
    "- **Output Scaling**: In practice, the output of the model's decision function (e.g., raw scores) may need to be scaled or calibrated to ensure that the margin and loss calculations are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Diverfence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler (KL) Divergence, also known as relative entropy, is a measure of how one probability distribution diverges from a second, expected probability distribution. It is often used in information theory and machine learning to quantify the difference between two probability distributions.\n",
    " logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For discrete distributions:\n",
    "$$ D_{KL}(P || Q) = \\sum_{x} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) $$\n",
    "\n",
    "For continuous distributions:\n",
    "$$ D_{KL}(P || Q) = \\int_{-\\infty}^{\\infty} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) dx $$\n",
    "\n",
    "In these formulas:\n",
    "- $$ D_{KL}(P || Q) $$ represents the KL Divergence from distribution  P  to distribution Q.\n",
    "-  P(x)  and Q(x)  are probability distributions over the variable  x .\n",
    "- log  represents the natural logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5ug9MEMWMNvjcRzgMc/P7",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
