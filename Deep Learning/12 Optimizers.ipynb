{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiajiId9rSFi"
   },
   "source": [
    "# **Optimizers**\n",
    "An optimizer is a function or an algorithm that adjusts the attributes of the neural network, such as weights and learning rates. Thus, it helps in reducing the overall loss and improving accuracy.\n",
    "\n",
    "## **Types of optimizers**\n",
    "- Batch GD\n",
    "- Mini Batch GD\n",
    "- Stochastic GD\n",
    "- Momentum\n",
    "- AdaGrad\n",
    "- NAG\n",
    "- Adam\n",
    "- RMSProp\n",
    "### Why we need other optimizers when we have these?\n",
    "\n",
    "**Challenges**\n",
    "- Learning rate (SOlution: LR Scheduling)\n",
    "- You have to minimize multiple weights. For 10 weights you will have 10 directions to move in. And we can not have seperate LR for each weight and bais.\n",
    "- Local Minima\n",
    "- Saddle point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylF6pDtMuNlG"
   },
   "source": [
    "## Exponentially Weighted Moving Average\n",
    "\n",
    "Is a technique using which you find trends in time series data.\n",
    "\n",
    "Used in:\n",
    "- Stock\n",
    "- Time series forcasting\n",
    "- Signal Process\n",
    "\n",
    "**How it work?**\n",
    "Every new data point have more weightage as compared to previous point. And every point weightage will reduce over time.\n",
    "\n",
    "$$ V_t = βV_(t-1) +(1-β)θ_t $$\n",
    "\n",
    "beta value is between 0 and 1\n",
    "\n",
    "\n",
    "|Date|Temp|\n",
    "|-------------|-------------|\n",
    "| 1 | 23|\n",
    "|2|12|\n",
    "|3|17|\n",
    "|4|19|\n",
    "\n",
    "Suppose beta is 0.9\n",
    "\n",
    "For $V_0 = θ_0$ Which is 23\n",
    "\n",
    "For $V_1$ :  $0.9*23 + 0.1(23) = 23 $\n",
    "\n",
    "For $V_2$ :  $0.9*23 + 0.1(12) = 21.9 $\n",
    "\n",
    "For $V_3$ :  $0.9*21.9 + 0.1(17) = 21.41 $\n",
    "\n",
    "For $V_4$ :  $0.9*21.41 + 0.1(19) = 21.16 $\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Impact of Beta value**\n",
    "\n",
    "If beta value is 0.9, it will behave as it is average of last ten days.\n",
    "if beta = 0.2\n",
    "$ β → \\frac{1}{1-β} → 10 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAh3uGJvzUaf"
   },
   "source": [
    "# **SGD with Momentum**\n",
    "## **The Why**\n",
    "\n",
    "In GD there was three problems:\n",
    "- Noisy Grad\n",
    "- Consistent grad\n",
    "- High curvature\n",
    "\n",
    "Momentum address all three problems\n",
    "\n",
    "## **The What**\n",
    "\n",
    "Consider you are moving from point A to point B but you don't know where exactly is point B. You will people where is it, if every person you asked pointed in one direction you will speed up in that direction. If people give mix direction your speed will be slower. This is the whole idea of momentum Optimizer.\n",
    "\n",
    "![alr](https://files.codingninjas.in/article_images/nesterov-accelerated-gradient-3-1644853109.webp)\n",
    "\n",
    "[link](https://www.codingninjas.com/studio/library/nesterov-accelerated-gradient)\n",
    "\n",
    "## **The How (Maths)**\n",
    "\n",
    "In normal optimizer we update weights with following equation:\n",
    "$$w_(t+1) = w_t - ηΔ w_t $$\n",
    "\n",
    "In momentum we take a term v which we will called velocity and instead of derivation we use velocity.$$w_(t+1) = w_t - v_t $$\n",
    "\n",
    "\n",
    "Here $v_t$ is calculated as: $$v_t = β v_(t-1) + η∇w_t$$\n",
    "\n",
    "Here $β$ value is between 0 and 1. $v_(t-1)$ is velocity at previous step. And second term is learning rate times current gradient.\n",
    "\n",
    "\n",
    "You are using history of velocity as momentum.\n",
    "\n",
    "### ***Effect of Beta***\n",
    "Beta is called Decaying factor.\n",
    "If Beta value is zero. Momentum will behave like SGD.\n",
    "\n",
    "\n",
    "If Beta value is one. There is no decay.\n",
    "\n",
    "\n",
    "## **Problem with momentum**\n",
    "\n",
    "It can be slow as compared to next optimizers. But will be fast than SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPpY9Z77p_wQ"
   },
   "source": [
    "# **Nesterov Accelerated Gradient (NAG)**\n",
    "\n",
    "![alr](https://files.codingninjas.in/article_images/nesterov-accelerated-gradient-4-1644853109.webp)\n",
    "\n",
    "[link](https://www.codingninjas.com/studio/library/nesterov-accelerated-gradient)\n",
    "\n",
    "$$w_(t+1) = w_t − v_t$$\n",
    "While calculating the $v_t$, We will include the look ahead gradient $(∇w_(la))$.\n",
    "$$v_t = β ·v_(t−1) + η∇w_(la)$$\n",
    "\n",
    "$∇w_(la)$ is calculated by:\n",
    "$$w_(la) = w_t − β · v_(t−1)$$\n",
    "\n",
    "This look-ahead gradient will be used in our update and will prevent overshooting.\n",
    "\n",
    "\n",
    "**Disadvantage**\n",
    "- You can get trapped in local minima\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "tf.keras.optimizers.SGD(\n",
    "                      learning_rate = 0.01,\n",
    "                      momentum = 0.0,\n",
    "                      nestoriv = Fasle,\n",
    "                      name = \"SGD\",\n",
    "                      **kwargs)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AdaGrad**\n",
    "\n",
    "- Adagrad is an optimization algorithm commonly used in training neural networks. It adapts the learning rate for each parameter based on the historical gradients, allowing for larger updates for infrequent parameters and smaller updates for frequent parameters. This adaptive learning rate can help improve convergence and training stability.\n",
    "\n",
    "## AdaGrad work better in the following cases:\n",
    "- Input features have different scale\n",
    "- Features are sparse\n",
    "\n",
    "\n",
    "**Enlongated Bowl Problem**\n",
    "\n",
    "In the context of machine learning and neural network training, the Elongated Bowl Problem typically arises when the optimization landscape has steep gradients in some directions and shallow gradients in others. This can occur due to various factors such as:\n",
    "\n",
    "**High Condition Number**: The condition number of the Hessian matrix (second-order derivatives of the loss function) is high, indicating that the curvature of the loss function varies significantly across different dimensions.\n",
    "\n",
    "**Correlated Features**: If the input features are highly correlated, it can lead to elongated contours in the optimization landscape.\n",
    "\n",
    "**Overparameterization**: Having a large number of parameters in the model can also contribute to the Enlongated Bowl Problem, as it increases the complexity of the optimization space.\n",
    "\n",
    "**Irrelevant Features**: Including irrelevant features in the model can create flat regions in the optimization landscape, exacerbating the elongated bowl shape.\n",
    "\n",
    "The Enlongated Bowl Problem can lead to difficulties in convergence and slow training of machine learning models. Gradient-based optimization algorithms may struggle to navigate the narrow valley of the loss function, resulting in slow progress or convergence to suboptimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1Rm1X2o8rOG3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\DL0\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9667 - loss: 0.2201\n",
      "Test Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adagrad optimizer\n",
    "optimizer = Adagrad(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm commonly used for training deep learning models. It combines the advantages of both AdaGrad and RMSProp algorithms by adapting the learning rates for each parameter based on the first and second moments of the gradients. Adam is known for its efficiency, robustness, and ability to handle various optimization landscapes effectively.\n",
    "\n",
    "Here's how Adam works and how you can implement it in a neural network using Keras with TensorFlow:\n",
    "\n",
    "**Adaptive Learning Rates**: Adam adapts the learning rates for each parameter by considering two main factors:\n",
    "\n",
    "    **First Moment (Mean)**: It calculates the exponentially decaying average of past gradients.\n",
    "    **Second Moment (Variance)**: It calculates the exponentially decaying average of past squared gradients.\n",
    "**Bias Correction**: Adam incorporates bias correction to account for the initialization bias of the first and second moment estimates, especially in the early stages of training.\n",
    "\n",
    "**Update Rule**: The update rule for Adam is defined as follows:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "m_t & = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\n",
    "v_t & = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n",
    "\\hat{m}_t & = \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "\\hat{v}_t & = \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_{t+1} & = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "   \n",
    "   $m_t$ and $v_t$ are the first and second moments of the gradients at time step $t$.\n",
    "   \n",
    "   $\\beta_1$ and $\\beta_2$ are exponential decay rates for the first and second moments (typically set to 0.9 and 0.999, respectively).\n",
    "  \n",
    "  $g_t$ is the gradient at time step $t$.\n",
    "  \n",
    "   $\\eta$ is the learning rate.\n",
    "   \n",
    "   $\\epsilon$ is a small constant (e.g., $10^{-7}$) to prevent division by zero.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0781\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
