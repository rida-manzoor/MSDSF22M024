{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXxnAKIOIZWxnVNylHc1KR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/23_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduced by [Cho, et al. in 2014, GRU (Gated Recurrent Unit)](https://arxiv.org/pdf/1412.3555)) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner."
      ],
      "metadata": {
        "id": "Lra02AurUTzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU are newer as compared to LSTM. GRU architecture is simpler than LSTM. Also they do not have sperate cell state. They only have hidden state.\n",
        "\n",
        "![gru](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-17-16-32-42.png)\n",
        "\n",
        "\n",
        "**Here you can think why we need GRU if LSTM were giving good results already?**\n",
        "* Lstm architecture is complex to understand as compared to GRU\n",
        "* Number of parameters is huge so it take more time in training\n",
        "\n",
        "We can say that GRU is solving this problem, they give slightly simpler architecture with less number of parameters, and also it take less time in training. There are certain datasets/problems where GRU outperform LSTMs. Also LSTM give good results on some datasets compared to GRU. So, if we are working with some problem we should try both.\n",
        "\n",
        "In summary, GRU networks are a type of RNN that use gating mechanisms to selectively update the hidden state at each time step, allowing them to effectively model sequential data. They have been shown to be effective in various natural language processing tasks, such as language modeling, machine translation, and speech recognition\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CuD4AbS3hMYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture\n",
        "\n",
        "![alr](https://www.researchgate.net/profile/Abien-Fred-Agarap/publication/319642918/figure/fig33/AS:631648458657812@1527608136275/Image-from-38-The-GRU-model-combines-the-forget-gate-and-input-gate-into-a-single.png)\n",
        "\n",
        "Here in above image there are following operations used:\n",
        "1. Plus Operation\n",
        "2. Sigmoid function\n",
        "3. Hadamard operation\n",
        "4. tanh function\n",
        "\n",
        "\n",
        "We only have two gates here unlike LSTM:\n",
        "* Update Gate\n",
        "* Reset Gate\n",
        "\n",
        "**GOAL**\n",
        "Main goal is to compute new hidden state h_t using last hidden state and input.\n",
        "\n",
        "\n",
        "Following are the notations we are using:\n",
        "\n",
        "* h_(t-1) Previous Hidden State\n",
        "\n",
        "* h_t Current hidden State\n",
        "\n",
        "* x_t Input\n",
        "\n",
        "* r_t Reset Gate\n",
        "\n",
        "* z_t Update Gate\n",
        "\n",
        "*  ̃ h_t  Candidate Hidden State\n",
        "\n",
        "All of them are vectors, dimesionality of all vectors will be same except input vector.\n",
        "\n",
        "\n",
        "**STEP**\n",
        "\n",
        "1. Calculate r_t\n",
        "2. Calculate candidate cell state\n",
        "3. Calculate z_t\n",
        "4. Calculate h_t\n",
        "\n",
        "\n",
        "Initially, we can say that above mention steps can be wrapped up in two steps/stages.\n",
        "\n",
        "h_t-1 → ̃h_t → h_t\n",
        "\n",
        "\n",
        "Transition from previous hidden state to candidate hidden state is done by reset gate and from candidate hidden state to current hidden state is done via update gate.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Xvbcnv3hocC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reset Gate\n",
        "\n",
        "Lets assume we have 4 dimensional r_t vector whose value is between 0 and 1.\n",
        "\n",
        "r_t = [0,0.3,0.5,1]\n",
        "\n",
        "If it is 0 that mean r_t is closed for that particular dimension. If it is 1 is that mean whole information is retained. We can say that r_t is used to decide how much past information to forget."
      ],
      "metadata": {
        "id": "BfwgOY8bgrRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Gate\n",
        "\n",
        "This gate update information\n",
        "\n",
        "\n",
        "## LSTM vs GRU\n",
        "1. **Number of gates**\n",
        "    * LSTM: input, output, and forget gate\n",
        "    * GRU: Reset and update gate\n",
        "2. **Memory units**\n",
        "    * LSTM: Use two seperate states: cell state and hidden state. Cell state acts as \"internal memory\" & is crucial for carrying long term dependencies.\n",
        "    * GRU: Only hidden state. It capture and output memory\n",
        "3. **Parameter Count**\n",
        "    * LSTM: Generally more parameter than GRU because of its additional gate and seperate cell state. For an input size of d and hidden size of h, LSTM has 4x((dxh)+(hxh)+h)4x((dxh)+(hxh)+h) parameters\n",
        "    * GRU: Has fewer parameters. For the same input, GRU has 3x((dxh)+(hxh)+h)3x((dxh)+(hxh)+h)\n",
        "4. **Computational Complexity**\n",
        "   * LSTM: More computationally expensive\n",
        "   * GRU: Simpler and can be faster to compute\n"
      ],
      "metadata": {
        "id": "7E2FBceat76D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdUwg9w2T7FH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "* https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
        "* https://www.geeksforgeeks.org/gated-recurrent-unit-networks/\n",
        "* https://www.analyticsvidhya.com/blog/2021/03/introduction-to-gated-recurrent-unit-gru/"
      ],
      "metadata": {
        "id": "YozQW9N_x2ni"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHflSAKeyRDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}