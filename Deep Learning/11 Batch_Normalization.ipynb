{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/Batch_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3QQP-ARd960"
   },
   "source": [
    "# Batch Normalization\n",
    "Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n",
    "\n",
    "- It consist of normalizaing activation vector from hidden layer using mean and variance of current batch. This normalization is applied before the non linear function.\n",
    "\n",
    "## Why Batch Normalization?\n",
    "\n",
    "\n",
    "The main reasons for using Batch Normalization are:\n",
    "\n",
    "1. **Stabilizing Learning:**\n",
    "   - Batch Normalization helps stabilize and accelerate the training of deep neural networks by normalizing the input to each layer.\n",
    "   - It reduces the internal covariate shift, which is the change in the distribution of the input to a neural network's layer during training. This can make training more challenging, especially in deeper networks.\n",
    "\n",
    "2. **Faster Convergence:**\n",
    "   - By normalizing the inputs, Batch Normalization reduces the risk of vanishing or exploding gradients. This enables the use of higher learning rates, leading to faster convergence during training.\n",
    "\n",
    "3. **Reduced Sensitivity to Initialization:**\n",
    "   - Batch Normalization reduces the sensitivity of a network to the choice of initial weights. This allows for more flexibility in choosing initial weights and simplifies the training process.\n",
    "\n",
    "4. **Regularization Effect:**\n",
    "   - Batch Normalization acts as a form of regularization by adding a slight noise to the activations during training. This noise can act as a form of implicit regularization, helping prevent overfitting.\n",
    "\n",
    "5. **Applicability to Various Activation Functions:**\n",
    "   - Batch Normalization works well with a variety of activation functions, including sigmoid, tanh, and rectified linear units (ReLU). This flexibility makes it suitable for different network architectures.\n",
    "\n",
    "6. **Facilitates Deeper Networks:**\n",
    "   - Batch Normalization enables the training of deeper neural networks by addressing issues such as vanishing gradients and *internal covariate shift*. Deeper networks can capture more complex features and relationships in data.\n",
    "\n",
    "\n",
    "\n",
    "**Covariant Shift**\n",
    "\n",
    "Covariate shift refers to a situation in which the distribution of input features (covariates) to a machine learning model changes between the training and testing phases. In other words, the relationship between the input variables and the target variable may differ between the training data and the data the model encounters during testing or deployment.\n",
    "\n",
    "\n",
    "**Internal COvariant shift**\n",
    "\n",
    "We define internal covariant shift as a change in the distribution of network activations due to change in network parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMPYc-Nah8zO"
   },
   "source": [
    "## **How Batch Normalization Work?**\n",
    "\n",
    "1. When we apply batch normalization, we use mini batch gradient descent.\n",
    "2. It is applied on each layer of NN (Can be applied to individual layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wVTIT6jmd9UH",
    "outputId": "7b11799a-caa1-4c97-fb48-f7d169dc2379"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>iq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.5</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.2</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.7</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa   iq  target\n",
       "0   3.0  120       0\n",
       "1   3.5  125       1\n",
       "2   4.0  130       0\n",
       "3   3.2  127       0\n",
       "4   3.7  132       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: make a data frame of two features cgpa and iq and target column is placed. Target column has two classes. There should be 5 rows in dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'cgpa': [3.0, 3.5, 4.0, 3.2, 3.7],\n",
    "    'iq': [120, 125, 130, 127, 132],\n",
    "    'target': [0, 1, 0, 0, 1]\n",
    "})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_05XyYk5jYOC"
   },
   "source": [
    "![alt](https://www.includehelp.com/python/images/one-hidden-layer-simplest-neural-network.jpg)\n",
    "\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "1. Calculate 'z'\n",
    "\n",
    "$$ z_1 = w_1 cgpa + w_2 Iq + b $$\n",
    "\n",
    "$$ z_2 = w_1 cgpa + w_2 Iq + b $$\n",
    "2. Calculate activation\n",
    "\n",
    "$$ a_11 = F(z_1) $$\n",
    "\n",
    "$$ a_12 = F(z_2) $$\n",
    "\n",
    "When Batch Normalization is applied to this hidden layer, it will normalize output of activation functions(Neurons) that means their mean will be zero and standard deviation is 1.\n",
    "\n",
    "1. Create mini batch of dataset\n",
    "2. Calculate z for each neuron\n",
    "3. Normalize each 'z' value\n",
    "\n",
    "$$μ_B = \\frac{1}{m}∑_{i=1}^{m}z^i$$\n",
    "\n",
    "$$ σ_B = \\sqrt(\\frac{1}{m}∑_{i=1}^{m}z^i-μ_B) $$\n",
    "\n",
    "$$ z_(11)^i = \\frac{z_(11)^i - μ_B}{σ_B + ϵ} $$\n",
    "\n",
    "$$ γz_(11)^i + β $$\n",
    "\n",
    "Here gama and beta are learnable parameters. In keras implementation, initial value of gamma is 1 and beta is 0.\n",
    "4. Calculate activation\n",
    "\n",
    "    a = g(z)\n",
    "\n",
    "\n",
    "> For each neuron batch normalization have 4 parameters, gama and beta are learnable parameters and mean and sigma are nonlearnable parameters.\n",
    "\n",
    "## During Test\n",
    "\n",
    "Exponentially weighted moving average (EWMA) maintained during training for each batch.\n",
    "\n",
    "\n",
    "## Advantages\n",
    "1. Training more stable\n",
    "2. Training faster\n",
    "3. It can act as regularizer.\n",
    "4. Weight init impact reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wRLJ8gA5d3Hc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\DL0\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9333 - loss: 0.2210\n",
      "Model with Batch Normalization - Test Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a model with Batch Normalization\n",
    "model_with_batchnorm = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),  # Batch Normalization layer after activation\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),  # Batch Normalization layer after activation\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Batch Normalization\n",
    "model_with_batchnorm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Batch Normalization\n",
    "model_with_batchnorm.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Evaluate the model with Batch Normalization on test data\n",
    "loss_with_batchnorm, accuracy_with_batchnorm = model_with_batchnorm.evaluate(X_test, y_test)\n",
    "print(f'Model with Batch Normalization - Test Accuracy: {accuracy_with_batchnorm:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrsFw0CDMglwT09rj8mmA1",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
