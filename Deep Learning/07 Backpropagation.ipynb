{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn-f2j36Ig8e"
   },
   "source": [
    "# BackPropagation\n",
    "\n",
    "Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights.\n",
    "\n",
    "To understand backpropagation, assume we are working on following data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2pcqPB5RDQf"
   },
   "source": [
    "| IQ | Cgpa |Lpa|\n",
    "|----|------|----|\n",
    "| 80 | 3 | 3\n",
    "| 70 | 3.7 | 6\n",
    "| 120 | 2.5 | 2\n",
    "\n",
    "Neural Netwrok we build for this problem have one hidden layer with two nodes and an output layer with one node. All nodes are using linear activation function. So following **steps** will be taken in order to train neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L77bL0YvSHD8"
   },
   "source": [
    "1. Initialize random weights and bais (We are initializing weights 1 and bais 0)\n",
    "2. Select a record (Assume we feed neural network with row one)\n",
    "3. Network will predict LPA (Forward Propagation)\n",
    "4. Choose a loss function (As it is regression problem, we use MSE)\n",
    "5. Update weights and bais (Using Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dYSZelISx3l"
   },
   "source": [
    "So Let's assume network give ŷ 9. Our error will be 36. To reduce error we have to reduce value of loss function. That's mean we have to adjust weights and bais.\n",
    "\n",
    "$$ w_(new) = w_(old) η \\frac{∂ L}{∂W_(old)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEEdb9u9gXFC"
   },
   "source": [
    "## **Memoization**\n",
    "\n",
    "In computing, memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls to pure functions and returning the cached result when the same inputs occur again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "7VNRcDIqhYwl"
   },
   "outputs": [],
   "source": [
    "def fab(n):\n",
    "  if n ==1 or n==0:\n",
    "    return 1\n",
    "  else:\n",
    "    return fab(n-1) + fab(n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omZiC1NxhoIC",
    "outputId": "7b0b3d6c-43a5-4956-cc5c-f3a28990d2ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63245986"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fab(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "bcTPSIVmhs4T"
   },
   "outputs": [],
   "source": [
    "def fab(n,d):\n",
    "  if n in d:\n",
    "    return d[n]\n",
    "  else:\n",
    "    d[n] = fab(n-1,d) + fab(n-2,d)\n",
    "    return d[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iu2n0JiiPx7",
    "outputId": "95ca9146-54b8-4d41-9ff5-53bc011daf85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63245986"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {0:1,1:1}\n",
    "fab(38,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3y5D3Esi0EU"
   },
   "source": [
    "**How we can use memoization in backpropagation?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnzwUu0tkEle"
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is one of the most popular algorithm to perform optimization. It is a way to minimize the loss function J(θ)\n",
    "\n",
    "\n",
    "**Types:** (Differs in how much data is used to compute gradient, trade off between accuracy and time)\n",
    "1: Batch Gradient Descent (Vanilla)\n",
    "\n",
    "Take entire dataset then make update\n",
    "\n",
    "2: Stochastic GD\n",
    "\n",
    "SGD uses only a randomly selected subset of the data for each iteration.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "epochs = 10\n",
    "for i in range (10):\n",
    "  shuffle data\n",
    "  for i in range(X.shape[0]):\n",
    "    select random point\n",
    "    ŷ (forward)\n",
    "    loss\n",
    "    w, b update\n",
    "    (average loss for each epoch)\n",
    "```\n",
    "\n",
    "> Which is faster?\n",
    "\n",
    "> Which will converge faster?\n",
    "\n",
    "3: Mini Batch SG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ4r7gVtoQbw"
   },
   "source": [
    "## Vanishing Gradient problem\n",
    "\n",
    "Vanishing gradient problem is a phenomenon that occurs during the training of deep neural networks, where the gradients that are used to update the network become extremely small or \"vanish\" as they are backpropogated from the output layers to the earlier layers.\n",
    "\n",
    "Because of :\n",
    "Sigmoid or tanh\n",
    "\n",
    "**How to recognize?**\n",
    "1. If loss is not reducing\n",
    "2. Plot weight graphs\n",
    "\n",
    "\n",
    "**How to handle Vinishing Gradient problem?**\n",
    "1. Reduce complexity of network\n",
    "2. Use ReLu Activation function\n",
    "3. Proper weight initialization\n",
    "4. Batch Normalization\n",
    "5. Using Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIGzSGSaqcEk"
   },
   "source": [
    "# ** How to improve Neural Network**\n",
    "\n",
    "1. Fine Tuning NN hyperparameter\n",
    "    1. No. of hidden layer\n",
    "    2. No. of neurons per layer\n",
    "    3. Learning rate\n",
    "    4. Optimizer\n",
    "    5. Batch size\n",
    "    6. Activation function\n",
    "    7. Epochs\n",
    "2. By Solving problems\n",
    "  1. Vanishing / Exploding Gradient problem\n",
    "  2. Not enough data\n",
    "  3. Slow training\n",
    "  4. Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAkdVzvbiXxu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMu7EWO0hpeEYQSH2vU5i3c",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
